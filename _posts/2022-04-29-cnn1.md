---
layout: post
title: "Convolutional Neural Networks - Part 1"
subtitle: "Analysis of pre-trained CNN model architectures and comparison of performance on
Food101 dataset"
background: '/img/posts/cnn/cnn.jpg'
---

# Convolutional Neural Networks - Part 1

This project analyses the different architecture, specificities, and historical advancements of 3 pre-trained Convolution Neural Network (CNN) models:
* Inception-ResNet v2
* MobileNetV3
* NASNet

By using transfer learning with these models, this project also aims to analyse and compare their performance and limitations when predicting 101 classes of food from the Food101 dataset.

### Introduction to Computer Vision and Convolutional Neural Networks
Convolutional neural networks have demonstrated excellent results in computer vision tasks like in image classification. Inspired by the optical and neural systems in humans, CNN’s involve a special type of multi-layer neural network (Boesch, 2022).

![figure 1](/img/posts/cnn/Picture1.png)
<span class="caption text-muted">Figure 1. Convolutional Neural Network Diagram</span>

Image classification is the task of categorizing images into one or multiple predefined classes (Boesch, 2022). Computer vision tasks like image classification, object detection, and image recognition are important for tasks in many different industries. Medical imaging, remote sensing, robotics, self-driving cars, face recognition, traffic control systems and many more all use this technology. Many real-world applications of computer vision require the recognition tasks to be carried out quickly and on computationally limited platforms (Howard et al., 2017). This is why the development of CNN’s that are more efficient and less computationally costly is so important.

### The Dataset


### Transfer Learning Background

##### Inception

##### MobileNet

##### NasNet


### The Transfer Learning Process

### Comparison of Results

The results show that the best performing model, out of the 3 tested, was MobileNetV3Large, with higher accuracy and lower loss for both training and validation.

<span class="caption text-muted">Table 1. Model Results of pre-trained models</span>

|       | InceptionResNetV2   | MobileNetV3   | NASNet   |
| ----------- | :----: | :----: | :----: |
| Training Accuracy      | 0.6246       | 0.6685       | 0.4881       |
| Validation Accuracy   | 0.6412        | 0.6944        | 0.5339        |
| Training Loss      | 1.4278       | 1.2489       | 2.0152       |
| Validation Loss   | 1.3267        | 1.1089        | 1.7762        |

The figures below show the training (blue) and validation (orange) accuracy and loss. The y
axis had limits set so that the scale would be comparable for the different models.

![figure ?](/img/posts/cnn/acc.png)
<span class="caption text-muted">Figure ?. Model Accuracy (left: InceptionResnetV2, middle: MobileNetV3, right: NASNet)</span>

![figure ?](/img/posts/cnn/loss.png)
<span class="caption text-muted">Figure ?. Model loss (left: InceptionResnetV2, middle: MobileNetV3, right: NASNet)</span>

The above figures reiterate the conclusion drawn from Table 1, that MobileNetV3 is the best performing model. It is interesting to note that the validation accuracy is higher than the training accuracy in models. I suspect a reason for this could be because the validation images were cleaned, and the training images weren’t, so they contained some noise (intense colours and some wrong labels). As this noise is representative of real-world data, it is good that it is included in the training set, because models built on this training data will do better in production. I also think that, with more epochs, the train and validation results will become very close, as the trend in the above plots I showing the lines moving closer as the epoch value increases.

As mentioned in the previous section, a big issue faced in this project was the computational limitations of using Google Collaboratory. The time limit on GPU use meant that 10 epochs was the maximum I could use whilst keeping the same number of images and level of data augmentation. Early in the modelling stage of this project, I experimented with training models on a smaller sample of data and found the accuracy continuing to increase (and the loss continuing to decrease) at least until 30 epochs, and likely beyond that point. So, for further work on this project, it would be recommended to use a program or computer where there aren’t time constraints on model training.
Another way to further improve on this project, would be to test out other pre-trained models, as there are many different ones available.