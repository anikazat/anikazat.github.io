---
layout: post
title: "Convolutional Neural Networks - Part 1"
subtitle: "Analysis of pre-trained CNN model architectures and comparison of performance on
Food101 dataset"
background: '/img/posts/cnn/cnn.jpg'
---

This project analyses the different architecture, specificities, and historical advancements of 3 pre-trained Convolution Neural Network (CNN) models:
* Inception-ResNet v2
* MobileNetV3
* NASNet

By using transfer learning with these models, this project also aims to analyse and compare their performance and limitations when predicting 101 classes of food from the Food101 dataset.

## Introduction to Computer Vision and Convolutional Neural Networks
Convolutional neural networks have demonstrated excellent results in computer vision tasks like in image classification. Inspired by the optical and neural systems in humans, CNN’s involve a special type of multi-layer neural network (Boesch, 2022).

![figure 1](/img/posts/cnn/Picture1.png)
<span class="caption text-muted">Figure 1. Convolutional Neural Network Diagram</span>

Image classification is the task of categorizing images into one or multiple predefined classes (Boesch, 2022). Computer vision tasks like image classification, object detection, and image recognition are important for tasks in many different industries. Medical imaging, remote sensing, robotics, self-driving cars, face recognition, traffic control systems and many more all use this technology. Many real-world applications of computer vision require the recognition tasks to be carried out quickly and on computationally limited platforms (Howard et al., 2017). This is why the development of CNN’s that are more efficient and less computationally costly is so important.

## The Dataset
The Food101 [dataset](https://www.kaggle.com/datasets/dansbecker/food-101) is a real-world (photos not taken under laboratory conditions) food image dataset which contains 101,000 images and 101 classes/food categories.
Each class has 750 training images (randomly sampled) that have not been cleaned, and 250 test images that have been manually cleaned (Bossard, 2014). Overall count: training = 75,750 images, and testing = 25,250 images.
The images in this dataset were rescaled to have a maximum side length of 512 pixels.

## Transfer Learning Background
Humans have the innate ability to transfer knowledge across tasks (i.e., we can use knowledge from one area and apply it to another related task). Meaning that we don’t have to always start from scratch. E.g., knowing how to ride a bike will make it easier to learning how to ride a motorbike.
In data science, transfer learning is a machine learning technique that allows data scientists to transfer knowledge gained from one machine learning task to another related task. The advantage of transfer learning is that it allows faster training and better results while using less data.

![figure 2](/img/posts/cnn/Picture2.png)
<span class="caption text-muted">Figure 2. Traditional Machine Learning vs Transfer Learning (Sarkar, 2018)</span>

The benefits of using transfer learning:
![figure 3](/img/posts/cnn/Picture3.png)
<span class="caption text-muted">Figure 3. The benefits of transfer learning [source: Machine Learning Applications and Trends: Algorithms, Methods, and Techniques (Olivas et al., 2010)]</span>

As shown in Figure 3, using transfer learning will give higher starting performance, higher learning rate during training, higher accuracy after training, and a faster training duration.

#### Inception – architectures, specificities, and historical advancements
**Version 1 (Inception-v1, also called GoogLeNet)** <br>
The Inception microarchitecture was introduced in 2014 by Szegedy et al.
The basis for creating the Inception model was to find a way to address some of the issues that are present in image classification tasks. Issues such as: being prone to overfitting in very deep networks and being computationally expensive

The main strength of this model architecture is how it improved the utilisation of the computing resources in the network (Szegedy et al. 2014). Szegedy et al. (2014) achieved this by designing a network that could get deeper and wider (rather than only getting deeper) while keeping the cost of computation the same. The diagram of the “naïve” inception model below (Figure 4) shows the “widening” of the network.

![figure 4](/img/posts/cnn/Picture4.png)
<span class="caption text-muted">Figure 4. Naïve inception module from Szegedy et al. (2014) paper</span>

Convolution is performed on the input, then there are filters of different sizes (1x1, 3x3, 5x5) and 3x3 max pooling, then the outputs are concatenated.

To keep the computational cost down, the number of input channels are limited. This is done by adding an extra 1x1 convolution before the 3x3 and 5x5 convolutions, as shown in Figure 5 below.

![figure 5](/img/posts/cnn/Picture5.png)
<span class="caption text-muted">Figure 5. Inception module with dimension reductions from Szegedy et al. (2014) paper</span>

Inception-v1 used 5 million parameters, whereas its predecessor (AlexNet) used 60 million parameters (12x the amount), meaning inception is better designed to perform well even under tight constraints on computational budget and memory (Szegedy et al., 2016). The lower computational cost with Inception compared with its competitors, means it can be utilised in big data tasks like mobile vision settings (Szegedy et al., 2016).

In the following years, several upgrades to the original Inception model were proposed which increased the accuracy and reduced the computational complexity. These upgrades are discussed below.

**Version 2 (Inception-v2)** <br>
The second version of Inception was designed to increase computational efficiency. With Inception-v2, Ioffe and Szegedy (2015) address a problem they call internal covariate shift (ICS). ICS is defined as the change in the distribution of network activations due to the change in network parameters during training. This issue slows down training by requiring lower learning rates and careful parameter initialization, making it difficult to train models with saturating nonlinearities. Ioffe and Szegedy (2015) proposed using batch normalisation (defined as making normalization a part of the model architecture and performing the normalization for each training mini-batch) to address to this problem. This allowed higher learning rates to be used. So, the training process was sped up and stabilised, and other regularisations (like dropout) could be reduced.

**Version 3 (Inception-v3)** <br>
The Inception-v3 architecture was published in 2015 in the same paper as version 2. This version explored ways to scale up networks and utilise the extra computation efficiently by factorising convolutions and using regularisation (Fang, 2020).

This model uses two ways of factorising convolutions: 
1. Factorising into smaller convolutions
    * 5×5 convolutional kernel is replaced by 2 3×3 convolutional kernel
2. Spatial Factorization into Asymmetric Convolutions
    * replace n × n convolution by a 1 × n convolution followed by a n × 1 convolution (computational cost saving increases dramatically as n grows)

**Version 4 (Inception-v4)** <br>
The goal with developing Inception-v4 was to see if Inception can be made to be more efficient by becoming deeper and wider. The architecture of version 4 is more uniform and simplified than the previous version. Moving the training set-up to TensorFlow allowed for this simplification of the architecture, as it removed the constraints brought on from the need for partitioning the model for distributed training using DistBelief (Szegedy et al., 2016). 



#### MobileNet

#### NasNet


## The Transfer Learning Process

## Comparison of Results

The results show that the best performing model, out of the 3 tested, was MobileNetV3Large, with higher accuracy and lower loss for both training and validation.

<span class="caption text-muted">Table 1. Model Results of pre-trained models</span>

|       | InceptionResNetV2   | MobileNetV3   | NASNet   |
| ----------- | :----: | :----: | :----: |
| Training Accuracy      | 0.6246       | 0.6685       | 0.4881       |
| Validation Accuracy   | 0.6412        | 0.6944        | 0.5339        |
| Training Loss      | 1.4278       | 1.2489       | 2.0152       |
| Validation Loss   | 1.3267        | 1.1089        | 1.7762        |

The figures below show the training (blue) and validation (orange) accuracy and loss. The y
axis had limits set so that the scale would be comparable for the different models.

![figure ?](/img/posts/cnn/acc.png)
<span class="caption text-muted">Figure ?. Model Accuracy (left: InceptionResnetV2, middle: MobileNetV3, right: NASNet)</span>

![figure ?](/img/posts/cnn/loss.png)
<span class="caption text-muted">Figure ?. Model loss (left: InceptionResnetV2, middle: MobileNetV3, right: NASNet)</span>

The above figures reiterate the conclusion drawn from Table 1, that MobileNetV3 is the best performing model. It is interesting to note that the validation accuracy is higher than the training accuracy in models. I suspect a reason for this could be because the validation images were cleaned, and the training images weren’t, so they contained some noise (intense colours and some wrong labels). As this noise is representative of real-world data, it is good that it is included in the training set, because models built on this training data will do better in production. I also think that, with more epochs, the train and validation results will become very close, as the trend in the above plots I showing the lines moving closer as the epoch value increases.

As mentioned in the previous section, a big issue faced in this project was the computational limitations of using Google Collaboratory. The time limit on GPU use meant that 10 epochs was the maximum I could use whilst keeping the same number of images and level of data augmentation. Early in the modelling stage of this project, I experimented with training models on a smaller sample of data and found the accuracy continuing to increase (and the loss continuing to decrease) at least until 30 epochs, and likely beyond that point. So, for further work on this project, it would be recommended to use a program or computer where there aren’t time constraints on model training.
Another way to further improve on this project, would be to test out other pre-trained models, as there are many different ones available.