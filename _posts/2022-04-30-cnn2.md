---
layout: post
title: "Convolutional Neural Networks - Part 2"
subtitle: "Pick the best pre-trained model from part 1 and fine-tune it on the Food101 Dataset"
background: '/img/posts/cnn/cnn.jpg'
---

The best performing model out of the 3 tested in this project was MobileNetV3Large. During the fine-tuning process, different numbers of frozen/unfrozen layers were tried out. From just 2 layers to around 25 layers. The final model has 240 layers frozen, and the remaining 22 unfrozen.
Consistent with the previous models, this model used the same top layers (global average pooling and 3 FC layers with 256, 128, and 101 neurons respectively)
In this fine-tuned model, there were 3,288,293 total parameters, with 1,089,301 trainable and 2,198,992 not trainable.

The rest of the model training approach remained the same as in the previous stage of the project. I tested out variation in data augmentation, but as it takes such a long for each epoch to train, the amount of testing I was able to do was limited.

## Analysis of fine-tuned models’ performance
The results from training this model show improvements in all measures by the fine-tuned model as compared to the previous model.

<span class="caption text-muted">Table 1. Comparison of models’ performance pre-trained model vs fine-tuned model</span>

|       | Pre-trained model   | Tuned Model   |
| ----------- | :----: | :----: |
| Training Accuracy      | 0.6685       | 0.7714       |
| Validation Accuracy   | 0.6944        | 0.7530        |
| Training Loss      | 1.2489       | 0.8221       |
| Validation Loss   | 1.1089        | 0.9013        |

![figure1](/img/posts/cnn/Picture 1.png)
<span class="caption text-muted">Figure 1. Fine-tuned model accuracy</span>

![figure2](/img/posts/cnn/Picture 2.png)
<span class="caption text-muted">Figure 2. Fine-tuned model loss</span>

Note that the figures above show only 5 epochs, this is because when the model was run with 10 epochs, the runtime kept disconnecting right at the end before the plots executed. But the results in table 4 were obtained from saving the models progress with the call back and finishing the training in a new run time.

As mentioned previously, it is recommended to use a higher number of epochs (>30) when training this model.

## Issues and Recommendations
The main issue that persisted throughout this project was the computational cost of running these models. Recommendations to address this would be either to use a different program or reduce the number of images in the dataset by random sampling n images from each class.

As well as training for more epochs, it would also be recommended to try adding different regularization techniques (e.g., dropouts) to see if this can further improve the accuracy of the models.



